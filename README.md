# ğŸ“š ScholarScout: Automated Academic Paper Analysis Pipeline

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4o-412991.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Active-success.svg)

A unified pipeline for automated academic research analysis: from paper collection to structured author affiliation extraction. Combines Google Scholar scraping with LLM-powered data mining to build comprehensive datasets for bibliometric analysis, collaboration mapping, and research trend discovery.

## ğŸ¯ Overview

ScholarScout provides an end-to-end solution for academic research analysis:

1. **Paper Collection** (`gs_MCP/`): Automated scraping and downloading from Google Scholar
2. **Data Extraction** (`main.py`): LLM-powered extraction of author affiliations and contact information
3. **Structured Output**: Clean CSV datasets ready for analysis

The system uses OpenAI's GPT-4o to intelligently parse author sections and preserve author-affiliation mappings even in complex multi-author papers.

## âœ¨ Features

### Paper Collection (gs_MCP)
- Automated Google Scholar scraping
- Bulk PDF download capability
- Organized storage in Papers directory

### Affiliation Extraction (main.py)
- **Batch Processing**: Handle 100+ papers automatically
- **Structured Extraction**: Author names, emails, departments, institutions, countries
- **Smart Mapping**: Preserves author-affiliation relationships (handles superscript notation)
- **Progress Tracking**: Real-time progress bars with failed paper reporting
- **CSV Export**: Clean, structured output for immediate analysis
- **Error Resilience**: Continues processing even when individual papers fail
- **Cost Efficient**: ~$1-3 for 100 papers using GPT-4o

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/Arupreza/ScholarScout.git
cd ScholarScout

# Install dependencies
pip install -r requirements.txt
```

### Setup

1. Create a `.env` file in the project root:

```env
OPENAI_API_KEY=sk-proj-your-api-key-here
```

2. Project structure:

```
ScholarScout/
â”œâ”€â”€ .venv/                          # Virtual environment
â”œâ”€â”€ gs_MCP/                         # Google Scholar scraper
â”œâ”€â”€ Papers/                         # Downloaded PDFs (auto-created)
â”œâ”€â”€ .env                           # API keys
â”œâ”€â”€ main.py                        # Affiliation extractor
â”œâ”€â”€ MCPrun.py                      # MCP runner
â”œâ”€â”€ papers_affiliations.csv        # Output
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

### Usage

#### Full Pipeline

```bash
# Step 1: Collect papers (using gs_MCP)
python MCPrun.py

# Step 2: Extract affiliations
python main.py
```

#### Affiliation Extraction Only

If you already have PDFs in the `Papers/` directory:

```bash
python main.py
```

The script will:
1. Scan the `Papers/` directory for PDF files
2. Extract text from first 3 pages (where author info typically appears)
3. Use GPT-4o to parse and structure author information
4. Save results to `papers_affiliations.csv`

### Custom Configuration

```python
from main import PaperAffilationExtractor

extractor = PaperAffilationExtractor(api_key="your-key", model="gpt-4o")

df = extractor.process_papers(
    papers_dir="./Papers",
    output_csv="custom_output.csv"
)
```

## ğŸ“Š Output Format

Generated CSV with the following structure:

| Column | Description | Example |
|--------|-------------|---------|
| `author_name` | Full name of the author | John Doe |
| `email` | Email address (if available) | john.doe@mit.edu |
| `department` | Department or division | Department of Computer Science |
| `institution` | University or organization | Massachusetts Institute of Technology |
| `country` | Country of affiliation | USA |
| `paper_name` | Source paper filename | transformers_attention_2017 |

### Sample Output

```csv
author_name,email,department,institution,country,paper_name
Ashish Vaswani,avaswani@google.com,Google Brain,Google,USA,attention_is_all_you_need
Noam Shazeer,noam@google.com,Google Brain,Google,USA,attention_is_all_you_need
Niki Parmar,nikip@google.com,Google Research,Google,USA,attention_is_all_you_need
```

## ğŸ“ˆ Performance Metrics

- **Success Rate**: 70-80% for standard academic papers
- **Processing Speed**: ~2 seconds per paper (with rate limiting)
- **Cost**: $0.01-0.03 per paper with GPT-4o
- **Accuracy**: 85-95% for clearly formatted papers

### Known Limitations

- Only extracts from first 3 pages (author info location)
- Depends on PDF text extraction quality
- May miss affiliations in footnotes or end-of-paper sections
- Conference proceedings often have less structured formats
- ~20-30% of papers may require manual review

## ğŸ› ï¸ Technical Details

### Architecture

```
Google Scholar â†’ gs_MCP â†’ Papers/ â†’ main.py â†’ CSV
  (Scraping)    (Download)  (PDFs)  (Extract) (Output)

Extraction Pipeline:
PDF File â†’ Text Extraction â†’ LLM Processing â†’ JSON Parsing â†’ DataFrame â†’ CSV
           (PyPDF2)         (GPT-4o API)      (Validation)   (Pandas)
```

### LLM Prompt Strategy

- **Few-shot learning**: Structured JSON output format
- **Context window**: First 8000 characters (covers most author sections)
- **Temperature**: 0 (deterministic output)
- **Response format**: Forced JSON mode for reliability

### Rate Limiting

Default: 0.5s delay between requests (120 papers/hour)

Adjust in `main.py`:
```python
time.sleep(0.5)  # Increase for lower tier limits
```

## ğŸ”§ Troubleshooting

### No text extracted from PDF
**Problem**: PDF is image-based or poorly scanned  
**Solution**: Use OCR preprocessing or switch to `pdfplumber`

### API rate limit errors
**Problem**: Too many requests  
**Solution**: Increase `time.sleep()` delay or upgrade OpenAI tier

### Malformed JSON errors
**Problem**: LLM returns invalid JSON  
**Solution**: Add retry logic with exponential backoff

### Missing emails
**Problem**: Not all authors have emails listed  
**Expected**: Common in academic papers (only corresponding author)

### gs_MCP connection issues
**Problem**: Google Scholar blocking requests  
**Solution**: Implement rate limiting, use proxies, or rotate user agents

## ğŸš§ Roadmap

### Extraction Module
- [ ] Retry logic with exponential backoff
- [ ] Parallel processing with rate limit semaphore
- [ ] OCR support for image-based PDFs
- [ ] Email validation and normalization
- [ ] Institution name standardization
- [ ] Checkpoint saving for large batches (1000+ papers)

### Collection Module
- [ ] Support for arXiv, PubMed, IEEE Xplore
- [ ] Advanced search filters
- [ ] Duplicate detection
- [ ] Metadata extraction during download

### Analysis Features
- [ ] Collaboration network visualization
- [ ] Geographic distribution mapping
- [ ] Institution ranking analysis
- [ ] Citation network building

## ğŸ“‚ Repository Structure

```
ScholarScout/
â”œâ”€â”€ .venv/                     # Virtual environment
â”œâ”€â”€ gs_MCP/                    # Google Scholar MCP server
â”‚   â””â”€â”€ [scraping modules]
â”œâ”€â”€ Papers/                    # Downloaded PDF papers
â”œâ”€â”€ .env                       # Environment variables (API keys)
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”œâ”€â”€ .python-version            # Python version specification
â”œâ”€â”€ main.py                    # Main affiliation extractor
â”œâ”€â”€ MCPrun.py                  # MCP runner script
â”œâ”€â”€ papers_affiliations.csv    # Extracted data output
â”œâ”€â”€ pyproject.toml             # Project configuration
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ uv.lock                    # Dependency lock file
```

## ğŸ“ Citation

If you use this tool in your research, please cite:

```bibtex
@software{scholarscout,
  title = {ScholarScout: Automated Academic Paper Analysis Pipeline},
  author = {Arupreza},
  year = {2025},
  url = {https://github.com/Arupreza/ScholarScout}
}
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details

## ğŸ¤ Contributing

Contributions welcome! Please open an issue or submit a pull request.

Areas for contribution:
- Additional paper sources (arXiv, PubMed, etc.)
- Improved PDF parsing methods
- Enhanced affiliation normalization
- Visualization tools
- Performance optimizations

## ğŸ“§ Contact

For questions or support, open an issue on GitHub.

---

**Built with**: OpenAI GPT-4o | Python 3.8+ | PyPDF2 | Pandas | Google Scholar MCP